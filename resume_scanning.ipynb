{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipogV70WBI8w"
      },
      "source": [
        "Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do4xXQ0FuIl9",
        "outputId": "8c063188-ae09-4f7f-e2fe-105038a0745f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-docx in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: PyMuPDF in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.23.11)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.3.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.9.1)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from PyMuPDF) (1.23.9)\n",
            "Requirement already satisfied: python-docx in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.3.0)\n",
            "Requirement already satisfied: python-magic in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.4.27)\n",
            "Requirement already satisfied: python-docx in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-docx) (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx PyMuPDF\n",
        "!pip install python-docx\n",
        "!pip install python-magic\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-_KTwy3z7JQ"
      },
      "source": [
        "Extract Text from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcV-Idk--BxU",
        "outputId": "2d6b1b47-c4eb-4c67-9c6f-ba4754d2b9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
            "Requirement already satisfied: spacy==2.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.3.5)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (7.4.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (1.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (2.28.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (63.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (4.64.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (0.7.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (3.0.9)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy==2.3.5) (1.0.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2022.9.14)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==2.3.5) (0.4.5)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "     ---------------------------------------- 12.0/12.0 MB 5.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from en-core-web-sm==2.3.1) (2.3.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.7)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (7.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.28.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.0.9)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.7.11)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.21.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (63.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2022.9.14)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (3.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<2.4.0,>=2.3.0->en-core-web-sm==2.3.1) (0.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install spacy==2.3.5\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k9MIO1Y3J-5",
        "outputId": "4515289d-1724-4bde-91a5-f953ebb0f22c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'docx'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docx'"
          ]
        }
      ],
      "source": [
        "import docx\n",
        "import magic\n",
        "import os\n",
        "import fitz\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.data.path.append('/path/to/your/nltk_data')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import numpy as np\n",
        "\n",
        "#Extract Text from files\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = ''\n",
        "    with fitz.open(pdf_path) as pdf_doc:\n",
        "        for page_num in range(pdf_doc.page_count):\n",
        "            page = pdf_doc[page_num]\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = docx.Document(docx_path)\n",
        "    text = ''\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "    return text\n",
        "\n",
        "def extract_text(file_path):\n",
        "    mime = magic.Magic()\n",
        "    file_type = mime.from_file(file_path)\n",
        "\n",
        "    if 'PDF' in file_type:\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif 'Microsoft Word' in file_type:\n",
        "        return extract_text_from_docx(file_path)\n",
        "    else:\n",
        "        return 'Unsupported file format'\n",
        "\n",
        "#clean Text data\n",
        "def clean_text(text):\n",
        "  '''\n",
        "  def remove_special_characters(text):\n",
        "      return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "  '''\n",
        "  def convert_to_lowercase(text):\n",
        "      return text.lower()\n",
        "\n",
        "\n",
        "\n",
        "  def remove_stop_words(text):\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = word_tokenize(text)\n",
        "      filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "      return ' '.join(filtered_text)\n",
        "\n",
        "\n",
        "  def lemmatize_text(text):\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      words = word_tokenize(text)\n",
        "      lemmatized_text = [lemmatizer.lemmatize(word) for word in words]\n",
        "      return ' '.join(lemmatized_text)\n",
        "\n",
        "\n",
        "  #cleaned_text = remove_special_characters(text)\n",
        "  cleaned_text = convert_to_lowercase(text)\n",
        "  cleaned_text = remove_stop_words(cleaned_text)\n",
        "  cleaned_text = lemmatize_text(cleaned_text)\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "#TF-IDF Score\n",
        "def score(resume_text,job_text):\n",
        "  if not resume_text.strip() or not job_text.strip():\n",
        "      print(\"No non-empty content to process.\")\n",
        "  else:\n",
        "      # Combine the texts for vectorization\n",
        "      content = [resume_text, job_text]\n",
        "\n",
        "      # Define a custom tokenizer function using NLTK\n",
        "      def custom_tokenizer(text):\n",
        "          # Your custom tokenization logic here\n",
        "          tokens = word_tokenize(text)\n",
        "          tokens = [re.sub('[^A-Za-z]', '', token).lower() for token in tokens]\n",
        "          return tokens\n",
        "\n",
        "      # Create TfidfVectorizer with custom tokenizer\n",
        "      tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "\n",
        "      # Fit and transform the data\n",
        "      tfidf_matrix = tfidf_vectorizer.fit_transform(content)\n",
        "\n",
        "      # Calculate cosine similarity\n",
        "      similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "      print(\"Cosine Similarity Matrix:\")\n",
        "      print(similarity_matrix)\n",
        "      score = similarity_matrix[0, 1]\n",
        "      return score\n",
        "\n",
        "\n",
        "\n",
        "def perform_lda(resume, job):\n",
        "    content = [resume, job]\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform(content)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "    lda.fit(X)\n",
        "    document_topics = lda.transform(X)\n",
        "\n",
        "    jaccard_similarity = 1 - np.abs(document_topics[0] - document_topics[1]).sum()\n",
        "\n",
        "    # Convert the Jaccard similarity to a range of [0, 1]\n",
        "    normalized_similarity = (jaccard_similarity + 1) / 2\n",
        "\n",
        "    return normalized_similarity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def final_model(resume_path,job_path):\n",
        "    resume_text = extract_text(resume_path)\n",
        "    job_text = extract_text(job_path)\n",
        "\n",
        "\n",
        "    cleaned_resume_text = clean_text(resume_text)\n",
        "    cleaned_job_text = clean_text(job_text)\n",
        "\n",
        "    lda_similarity = perform_lda(resume_text, job_text)\n",
        "    tf_idf_Score = score(cleaned_resume_text,cleaned_job_text)\n",
        "    print(lda_similarity,tf_idf_Score)\n",
        "    return round((lda_similarity*0.3+tf_idf_Score*0.7)*100,2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CGJtTxWYxmU"
      },
      "outputs": [],
      "source": [
        "resume_path = 'D:/FOLDER/final year project/project/Major Project/resources/Jobs/A career in our Cybersecurity.docx'\n",
        "job_path = 'D:/FOLDER/final year project/project/Major Project/resources/Resumes/sample_resume1.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR8KvxGp0iwI",
        "outputId": "40b4806c-8226-47d4-8367-63ac27dfed94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity Matrix:\n",
            "[[1.         0.72632921]\n",
            " [0.72632921 1.        ]]\n",
            "0.998664793902435 0.7263292066941753\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "80.8"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_model(resume_path,job_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FwBzKg5gWcQ"
      },
      "outputs": [],
      "source": [
        "def calculate_score(resume_path, job_desc_path):\n",
        "    # Your scoring logic goes here\n",
        "    # This is just a placeholder, you may want to implement a more sophisticated scoring algorithm\n",
        "    score = 0\n",
        "    # Example: Count the number of common keywords or match certain criteria\n",
        "    common_keywords = set(resume_path.lower().split()) & set(job_desc_path.lower().split())\n",
        "    score = len(common_keywords)\n",
        "    return score\n",
        "\n",
        "def score_all_combinations(resume_folder, job_desc_folder):\n",
        "    # Get the list of resume and job description files\n",
        "    resume_files = os.listdir(resume_folder)\n",
        "    job_desc_files = os.listdir(job_desc_folder)\n",
        "\n",
        "    # Iterate through each combination of resume and job description files\n",
        "    for resume_file in resume_files:\n",
        "        for job_desc_file in job_desc_files:\n",
        "            # Construct the full paths for each file\n",
        "            resume_path = os.path.join(resume_folder, resume_file)\n",
        "            job_desc_path = os.path.join(job_desc_folder, job_desc_file)\n",
        "\n",
        "            # Calculate the score for the current combination\n",
        "            score = calculate_score(resume_path, job_desc_path)\n",
        "\n",
        "            # Print or store the score as needed\n",
        "            print(f\"Score for {resume_file} and {job_desc_file}: {score}\")\n",
        "\n",
        "# Specify the paths to your resume and job description folders\n",
        "resume_folder_path = \"/path/to/resume/folder\"\n",
        "job_desc_folder_path = \"/path/to/job/description/folder\"\n",
        "\n",
        "# Call the function to score all combinations\n",
        "score_all_combinations(resume_folder_path, job_desc_folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "function score(resume_text, job_text):\n",
        "\n",
        "# Step 2: Combine Texts\n",
        "content = [resume_text, job_text]\n",
        "\n",
        "# Step 3: Define Custom Tokenizer\n",
        "def custom_tokenizer(text):\n",
        "    # Your custom tokenization logic here\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [re.sub('[^A-Za-z]', '', token).lower() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Step 4: Create TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
        "\n",
        "# Step 5: Fit and Transform Data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(content)\n",
        "\n",
        "# Step 6: Calculate Cosine Similarity\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "score = similarity_matrix[0, 1]\n",
        "Display the results\n",
        "print(score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
